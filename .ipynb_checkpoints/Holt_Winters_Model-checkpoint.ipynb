{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holt Winters Exponential Smoothing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "def collect_data():\n",
    "\n",
    "    # Data from the John Hopkins University Dataset on GitHub\n",
    "    # https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series\n",
    "\n",
    "    # Defining the variables required\n",
    "    filenames = ['time_series_covid19_confirmed_global.csv',\n",
    "                'time_series_covid19_deaths_global.csv',\n",
    "                'time_series_covid19_recovered_global.csv']\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "\n",
    "    # Making the main dataframes required for the analysis\n",
    "    confirmed_global = pd.read_csv(url + filenames[0])\n",
    "    deaths_global = pd.read_csv(url + filenames[1])\n",
    "    recovered_global = pd.read_csv(url + filenames[2])\n",
    "    country_cases = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_country.csv')\n",
    "\n",
    "    # Simple Data Cleaning - Removing and renaming the Columns\n",
    "\n",
    "    # Removing the Province/State column, as it is pretty much not of any use\n",
    "    confirmed_global.drop(columns = ['Province/State', 'Lat', 'Long'], inplace = True)\n",
    "    deaths_global.drop(columns = ['Province/State', 'Lat', 'Long'], inplace = True)\n",
    "    recovered_global.drop(columns = ['Province/State', 'Lat', 'Long'], inplace = True)\n",
    "    country_cases.drop(columns = ['Last_Update', 'Incident_Rate', 'People_Tested', 'People_Hospitalized', 'UID'], inplace = True)\n",
    "    # Renaming the columns for easier access\n",
    "    confirmed_global.rename(columns = {\"Country/Region\": \"country\"}, inplace = True)\n",
    "    deaths_global.rename(columns = {\"Country/Region\": \"country\"}, inplace = True)\n",
    "    recovered_global.rename(columns = {\"Country/Region\": \"country\"}, inplace = True)\n",
    "\n",
    "    country_cases.rename(columns = {\n",
    "        \"Country_Region\" : \"country\",\n",
    "        \"Confirmed\": \"confirmed\",\n",
    "        \"Deaths\": \"deaths\",\n",
    "        \"Recovered\" : \"recovered\",\n",
    "        \"Active\" : \"active\",\n",
    "        \"Mortality_Rate\": \"mortality\"\n",
    "    }, inplace = True)\n",
    "\n",
    "    # Removing some duplicate values from the table\n",
    "    confirmed_global = confirmed_global.groupby(['country'], as_index = False).sum()\n",
    "    deaths_global = deaths_global.groupby(['country'], as_index = False).sum()\n",
    "    recovered_global = recovered_global.groupby(['country'], as_index = False).sum()\n",
    "\n",
    "    # This value is being changed as there was an error in the original dataset that had to be modified\n",
    "    confirmed_global.at[178, '5/20/20'] = 251667\n",
    "\n",
    "    return (confirmed_global, deaths_global, recovered_global, country_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_global, deaths_global, recovered_global, country_cases = collect_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_cases(country):\n",
    "    time_series = confirmed_global.melt(id_vars = ['country'], var_name = 'date', value_name = 'cases')\n",
    "    time_series = time_series[time_series['country'] == country]\n",
    "    time_series = time_series.drop(['country'], axis = 1)\n",
    "    time_series.index = [x for x in range(len(time_series))]\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(country_name):\n",
    "    # Getting the confirmed cases of that country\n",
    "    cases = get_new_cases(country_name)\n",
    "    \n",
    "    # Removing the zero values \n",
    "    is_0 = cases['cases'] != 0\n",
    "    cases = cases[is_0]\n",
    "    \n",
    "    # Making the training and test sets\n",
    "    split_ratio = 0.80\n",
    "    train_size = int(split_ratio * len(cases))\n",
    "    train_df, test_df = cases.iloc[:train_size, :], cases.iloc[train_size:, :]\n",
    "    train_df, test_df = pd.Series(train_df['cases'].values, train_df['date']), pd.Series(test_df['cases'].values, test_df['date'])\n",
    "    cases = pd.Series(cases['cases'].values, cases['date'])\n",
    "    \n",
    "    # Model\n",
    "    model = ExponentialSmoothing(train_df, trend = 'add', seasonal = 'mul', freq = 'D').fit(method = \"Powell\")\n",
    "    forecast1 = model.forecast(len(test_df))\n",
    "    \n",
    "    print(mape(forecast1, test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/statsmodels/tsa/holtwinters/model.py:427: FutureWarning: After 0.13 initialization must be handled at model creation\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/scipy/optimize/_minimize.py:521: RuntimeWarning: Method Powell cannot handle constraints nor bounds.\n",
      "  warn('Method %s cannot handle constraints nor bounds.' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.342687394275453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/statsmodels/tsa/holtwinters/model.py:920: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict('India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/statsmodels/tsa/holtwinters/model.py:427: FutureWarning: After 0.13 initialization must be handled at model creation\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/scipy/optimize/_minimize.py:521: RuntimeWarning: Method Powell cannot handle constraints nor bounds.\n",
      "  warn('Method %s cannot handle constraints nor bounds.' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7236136986660006\n"
     ]
    }
   ],
   "source": [
    "predict('Russia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_holt(country_name, sl, st):\n",
    "    # Getting the confirmed cases of that country\n",
    "    cases = get_new_cases(country_name)\n",
    "    \n",
    "    # Removing the zero values \n",
    "    is_0 = cases['cases'] != 0\n",
    "    cases = cases[is_0]\n",
    "    \n",
    "    # Making the training and test sets\n",
    "    split_ratio = 0.80\n",
    "    train_size = int(split_ratio * len(cases))\n",
    "    train_df, test_df = cases.iloc[:train_size, :], cases.iloc[train_size:, :]\n",
    "    train_df, test_df = pd.Series(train_df['cases'].values, train_df['date']), pd.Series(test_df['cases'].values, test_df['date'])\n",
    "    cases = pd.Series(cases['cases'].values, cases['date'])\n",
    "    \n",
    "    # Model\n",
    "    model = ExponentialSmoothing(train_df, trend = 'add', seasonal = 'mul', freq = 'D').fit(smoothing_level = sl, smoothing_trend = st, method = \"Powell\")\n",
    "    forecast1 = model.forecast(len(test_df))\n",
    "    \n",
    "    return (mape(forecast1, test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.018238506600422 0.7 0.6\n"
     ]
    }
   ],
   "source": [
    "# Custom grid search\n",
    "sl = [i/10 for i in range(1, 11)]\n",
    "st = [i/10 for i in range(1, 11)]\n",
    "lowest = 100\n",
    "sl_l = None\n",
    "st_l = None\n",
    "\n",
    "for sli in sl:\n",
    "    for sti in st:\n",
    "        ma = grid_search_holt('India', sli, sti)\n",
    "        if ma <= lowest:\n",
    "            lowest = ma\n",
    "            sl_l = sli\n",
    "            st_l = sti\n",
    "print(lowest, sl_l, st_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
